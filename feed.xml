<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://amgadmadkour.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://amgadmadkour.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-05T00:39:59+00:00</updated><id>https://amgadmadkour.com/feed.xml</id><title type="html">Amgad Madkour</title><subtitle></subtitle><entry><title type="html">Review of ModernBert</title><link href="https://amgadmadkour.com/blog/2025/modernbert/" rel="alternate" type="text/html" title="Review of ModernBert"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://amgadmadkour.com/blog/2025/modernbert</id><content type="html" xml:base="https://amgadmadkour.com/blog/2025/modernbert/"><![CDATA[<p>ModernBERT is a robust and versatile language model designed for natural language understanding tasks. It leverages the power of transformer architecture to provide state-of-the-art performance in various NLP applications. In this notebook, we will demonstrate several tasks that can be performed using the ModernBERT model, including masked language modeling, feature extraction, sentence similarity, and next-word prediction. These tasks will showcase the capabilities of ModernBERT without requiring any fine-tuning, highlighting its effectiveness in handling diverse language processing challenges right out of the box.</p> <h2 id="install-dependencies">Install Dependencies</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">!</span>uv pip <span class="nb">install </span>torch torchvision torchaudio <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu126
<span class="o">!</span>uv pip <span class="nb">install </span>git+https://github.com/huggingface/transformers.git
<span class="o">!</span>uv pip <span class="nb">install </span>skikit-learn
</code></pre></div></div> <h2 id="masked-language-modeling-fill-mask-task">Masked Language Modeling (Fill-Mask Task)</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize the fill-mask pipeline
</span><span class="n">fill_mask</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">fill-mask</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">#Example masked text
</span><span class="n">masked_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">The capital of France is [MASK].</span><span class="sh">"</span>

<span class="c1">#Get predictions for the masked token
</span><span class="n">predictions</span> <span class="o">=</span> <span class="nf">fill_mask</span><span class="p">(</span><span class="n">masked_text</span><span class="p">)</span>

<span class="c1"># Display predictions
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Masked Text:</span><span class="sh">"</span><span class="p">,</span> <span class="n">masked_text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictions:</span><span class="sh">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  - </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">sequence</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> (score: </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output">Output</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Masked Text: The capital of France is <span class="o">[</span>MASK].

Predictions:
The capital of France is Paris. <span class="o">(</span>score: 0.9233<span class="o">)</span>
The capital of France is Lyon. <span class="o">(</span>score: 0.0359<span class="o">)</span>
The capital of France is Nancy. <span class="o">(</span>score: 0.0231<span class="o">)</span>
The capital of France is Nice. <span class="o">(</span>score: 0.0062<span class="o">)</span>
The capital of France is Orleans. <span class="o">(</span>score: 0.0026<span class="o">)</span>
</code></pre></div></div> <h2 id="feature-extraction">Feature Extraction</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize the feature extraction pipeline
</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">feature-extraction</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Example text
</span><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT is a robust model for natural language understanding.</span><span class="sh">"</span>

<span class="c1"># Extract features
</span><span class="n">features</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Display feature dimensions
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Extracted feature shape: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output-1">Output</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracted feature shape: 1 x 14
</code></pre></div></div> <h2 id="sentence-similarity">Sentence Similarity</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="n">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="c1"># Initialize the feature extraction pipeline
</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">feature-extraction</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Example sentences
</span><span class="n">sentence_1</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT is a great language model.</span><span class="sh">"</span>
<span class="n">sentence_2</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT excels in understanding language.</span><span class="sh">"</span>

<span class="c1"># Extract embeddings
</span><span class="n">embedding_1</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">sentence_1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">embedding_2</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">sentence_2</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Compute cosine similarity
</span><span class="n">similarity</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">([</span><span class="n">embedding_1</span><span class="p">],</span> <span class="p">[</span><span class="n">embedding_2</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Similarity between sentences: </span><span class="si">{</span><span class="n">similarity</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output-2">Output</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Similarity between sentences: 0.9572
</code></pre></div></div> <h2 id="next-word-prediction">Next Word Prediction</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize the fill-mask pipeline
</span><span class="n">fill_mask</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">fill-mask</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Example text with a masked token at the end
</span><span class="n">masked_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT is designed for [MASK].</span><span class="sh">"</span>

<span class="c1"># Get predictions
</span><span class="n">predictions</span> <span class="o">=</span> <span class="nf">fill_mask</span><span class="p">(</span><span class="n">masked_text</span><span class="p">)</span>

<span class="c1"># Display next-word predictions
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Masked Text:</span><span class="sh">"</span><span class="p">,</span> <span class="n">masked_text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next Word Predictions:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  - </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">sequence</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> (score: </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="conclusion">Conclusion</h3> <p>Four approaches were presented of how ModernBERT can be used to perform various natural language processing tasks without requiring any fine-tuning. Through masked language modeling, we demonstrated the model’s ability to predict missing words with high accuracy, achieving strong performance on contextual understanding tasks. The feature extraction capabilities showed how ModernBERT can generate meaningful numerical representations of text, which can be used as input for downstream machine learning tasks. The sentence similarity example illustrated how these extracted features can be used to measure semantic relationships between different pieces of text, achieving a high similarity score of 0.9572 for semantically related sentences. Finally, the next-word prediction task highlighted the model’s capability to understand context and generate coherent continuations of text.</p>]]></content><author><name></name></author><category term="nlp"/><category term="introduction"/><category term="review"/><category term="encoder"/><summary type="html"><![CDATA[ModernBERT is a robust and versatile language model designed for natural language understanding tasks. It leverages the power of transformer architecture to provide state-of-the-art performance in various NLP applications. In this notebook, we will demonstrate several tasks that can be performed using the ModernBERT model, including masked language modeling, feature extraction, sentence similarity, and next-word prediction. These tasks will showcase the capabilities of ModernBERT without requiring any fine-tuning, highlighting its effectiveness in handling diverse language processing challenges right out of the box.]]></summary></entry><entry><title type="html">Introduction to Machine Learning</title><link href="https://amgadmadkour.com/blog/2024/ml-intro/" rel="alternate" type="text/html" title="Introduction to Machine Learning"/><published>2024-07-24T00:00:00+00:00</published><updated>2024-07-24T00:00:00+00:00</updated><id>https://amgadmadkour.com/blog/2024/ml-intro</id><content type="html" xml:base="https://amgadmadkour.com/blog/2024/ml-intro/"><![CDATA[<h2 id="what-is-machine-learning">What is Machine Learning?</h2> <blockquote> <p>Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959</p> </blockquote> <p>From an engineering perspective, machine learning can be defined as:</p> <ul> <li><strong>P</strong>: Performance</li> <li><strong>T</strong>: Task</li> <li><strong>E</strong>: Experience</li> </ul> <p>For example, a <strong>spam filter</strong> is a <em>task</em>, and the <em>performance</em> is the <strong>accuracy</strong> of the filter. The <em>experience</em> is the <strong>training data</strong> that the filter is trained on. The goal of machine learning is to improve the performance of a task by learning from experience. In the case of the spam filter, the goal is to improve the accuracy of the filter by learning from training data.</p> <p>Machine learning is suitable for the following problems</p> <ul> <li>Require a lot of hand-tuning or long lists of rules.</li> <li>Complex where traditional methods do not work well.</li> <li>Fluctuating over time where the data changes frequently.</li> </ul> <p>The alternative to machine learning is <strong>hand-crafted rules</strong>. For example, a spam filter could be built by hand-crafting rules such as:</p> <ul> <li>If the email contains the word “coupon”, then it is spam.</li> <li>If the email is from a known spammer, then it is spam.</li> <li>If the email is from a friend, then it is not spam.</li> </ul> <p>The problem with hand-crafted rules is that they are <strong>brittle</strong>. For example, if the word “coupon” is replaced with “koupon”,then the rule will not work. Machine learning is more robust because it learns which words are associated with spam from the data.</p> <h2 id="components">Components</h2> <p>The three major components of a machine learning system are:</p> <ul> <li><a href="http://amgadmadkour.com/notes/ml/concept/data">Data</a></li> <li><a href="http://amgadmadkour.com/notes/ml/concept/model">Model</a></li> <li><a href="http://amgadmadkour.com/notes/ml/concept/learning">Learning</a></li> </ul> <h2 id="types-of-learning">Types of Learning</h2> <p>There are several ways to categorize machine learning algorithms. One way is to categorize them by the type of learning they use. The following is a list of the types of learning:</p> <ol> <li>Trained using human <strong>supervision</strong>. <ol> <li>Supervised Learning: The training data is labeled.</li> <li>Unsupervised Learning: The training data is unlabeled.</li> <li>Semi-supervised Learning: The training data is partially labeled.</li> <li>Reinforcement Learning: The training data is the result of an agent interacting with an environment.</li> </ol> </li> <li>Learn by <strong>detecting patterns</strong>. <ol> <li>Instance-based Learning: The system learns the training data and uses a similarity measure to generalize to new instances.</li> <li>Model-based Learning: The system learns a model of the training data and uses the model to generalize to new instances.</li> </ol> </li> <li>Learn <strong>incrementally</strong> or <strong>on the fly</strong>. <ol> <li>Online Learning: The system learns incrementally from a stream of data.</li> <li>Batch Learning: The system learns from a fixed dataset.</li> </ol> </li> </ol> <h2 id="pillars-of-machine-learning">Pillars of Machine Learning</h2> <ul> <li>Classification</li> <li>Regression</li> <li>Dimensionality Reduction</li> <li>Density Estimation</li> </ul>]]></content><author><name></name></author><category term="machinelearning"/><category term="introduction"/><summary type="html"><![CDATA[What is Machine Learning?]]></summary></entry></feed>