<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>ModernBERT is a robust and versatile language model designed for natural language understanding tasks. It leverages the power of transformer architecture to provide state-of-the-art performance in various NLP applications. In this post, we will illustrate several tasks that can be performed using the ModernBERT model, including masked language modeling, feature extraction, sentence similarity, and next-word prediction. These tasks will showcase the capabilities of ModernBERT without requiring any fine-tuning, highlighting its effectiveness in handling diverse language processing challenges right out of the box.</p> <h2 id="install-dependencies">Install Dependencies</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">!</span>uv pip <span class="nb">install </span>torch torchvision torchaudio <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu126
<span class="o">!</span>uv pip <span class="nb">install </span>git+https://github.com/huggingface/transformers.git
<span class="o">!</span>uv pip <span class="nb">install </span>skikit-learn
</code></pre></div></div> <h2 id="masked-language-modeling-fill-mask-task">Masked Language Modeling (Fill-Mask Task)</h2> <p>This example demonstrates ModernBERT’s <strong>contextual understanding</strong> capabilities, showing how the model can predict missing words based on surrounding context.</p> <pre><code class="language-mermaid">flowchart LR
    A["Input"] --&gt; B["ModernBERT&lt;br/&gt;Model"]
    B --&gt; C["Contextual&lt;br/&gt;Analysis"]
    C --&gt; D["Multiple&lt;br/&gt;Predictions"]
    D --&gt; E["Ranked Results:&lt;br/&gt;Paris (92.33%)&lt;br/&gt;Lyon (3.59%)&lt;br/&gt;Nancy (2.31%)"]

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#fff8e1
</code></pre> <p>The key aspects illustrated include:</p> <ul> <li> <strong>Multi-candidate Prediction</strong>: The model provides multiple predictions ranked by confidence scores</li> <li> <strong>High Accuracy</strong>: Achieves 92.33% confidence for the correct answer in context</li> <li> <strong>Zero-shot Performance</strong>: Works without any fine-tuning on the specific task</li> <li> <strong>Bidirectional Understanding</strong>: Leverages both left and right context to make predictions</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize the fill-mask pipeline
</span><span class="n">fill_mask</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">fill-mask</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">#Example masked text
</span><span class="n">masked_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">The capital of France is [MASK].</span><span class="sh">"</span>

<span class="c1">#Get predictions for the masked token
</span><span class="n">predictions</span> <span class="o">=</span> <span class="nf">fill_mask</span><span class="p">(</span><span class="n">masked_text</span><span class="p">)</span>

<span class="c1"># Display predictions
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Masked Text:</span><span class="sh">"</span><span class="p">,</span> <span class="n">masked_text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictions:</span><span class="sh">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  - </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">sequence</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> (score: </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output">Output</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Masked Text: The capital of France is <span class="o">[</span>MASK].

Predictions:
The capital of France is Paris. <span class="o">(</span>score: 0.9233<span class="o">)</span>
The capital of France is Lyon. <span class="o">(</span>score: 0.0359<span class="o">)</span>
The capital of France is Nancy. <span class="o">(</span>score: 0.0231<span class="o">)</span>
The capital of France is Nice. <span class="o">(</span>score: 0.0062<span class="o">)</span>
The capital of France is Orleans. <span class="o">(</span>score: 0.0026<span class="o">)</span>
</code></pre></div></div> <h2 id="feature-extraction">Feature Extraction</h2> <p>This example showcases ModernBERT’s ability to <strong>encode semantic meaning</strong> into numerical representations.</p> <p>The key aspects demonstrated include:</p> <ul> <li> <strong>Numerical Representation</strong>: Converts text into dense numerical vectors (embeddings)</li> <li> <strong>Dimensionality Transparency</strong>: Shows the output shape for understanding data structure</li> <li> <strong>Versatility</strong>: These features serve as input for downstream machine learning tasks</li> <li> <strong>Sentence-level Encoding</strong>: Demonstrates meaningful representation of entire sentences</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize the feature extraction pipeline
</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">feature-extraction</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Example text
</span><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT is a robust model for natural language understanding.</span><span class="sh">"</span>

<span class="c1"># Extract features
</span><span class="n">features</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Display feature dimensions
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Extracted feature shape: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output-1">Output</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracted feature shape: 1 x 14
</code></pre></div></div> <h2 id="sentence-similarity">Sentence Similarity</h2> <p>This example illustrates ModernBERT’s capability to <strong>measure semantic relationships</strong> between different pieces of text.</p> <pre><code class="language-mermaid">flowchart LR
    A["Sentence 1:&lt;br/&gt;ModernBERT is a great&lt;br/&gt;language model."] --&gt; C["Feature&lt;br/&gt;Extraction"]
    B["Sentence 2:&lt;br/&gt;ModernBERT excels in&lt;br/&gt;understanding language."] --&gt; C
    C --&gt; D["Embedding 1&lt;br/&gt;(Vector)"]
    C --&gt; E["Embedding 2&lt;br/&gt;(Vector)"]
    D --&gt; F["Cosine&lt;br/&gt;Similarity"]
    E --&gt; F
    F --&gt; G["Similarity Score:&lt;br/&gt;0.9572&lt;br/&gt;(95.72% similar)"]

    style A fill:#e1f5fe
    style B fill:#e1f5fe
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    style F fill:#e8f5e8
    style G fill:#fff8e1
</code></pre> <p>The key aspects highlighted include:</p> <ul> <li> <strong>Semantic Relationship Measurement</strong>: Uses extracted embeddings to compute similarity between sentences</li> <li> <strong>Quantitative Analysis</strong>: Employs standard similarity metrics (cosine similarity) for measurable results</li> <li> <strong>High Correlation Detection</strong>: Achieves 0.9572 similarity score for semantically related sentences</li> <li> <strong>Practical Application</strong>: Shows real-world usage for similarity tasks and semantic search</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="n">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="c1"># Initialize the feature extraction pipeline
</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">feature-extraction</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Example sentences
</span><span class="n">sentence_1</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT is a great language model.</span><span class="sh">"</span>
<span class="n">sentence_2</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT excels in understanding language.</span><span class="sh">"</span>

<span class="c1"># Extract embeddings
</span><span class="n">embedding_1</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">sentence_1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">embedding_2</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">sentence_2</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Compute cosine similarity
</span><span class="n">similarity</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">([</span><span class="n">embedding_1</span><span class="p">],</span> <span class="p">[</span><span class="n">embedding_2</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Similarity between sentences: </span><span class="si">{</span><span class="n">similarity</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output-2">Output</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Similarity between sentences: 0.9572
</code></pre></div></div> <h2 id="next-word-prediction">Next Word Prediction</h2> <p>This example demonstrates ModernBERT’s <strong>generative capabilities</strong> and context-aware text completion.</p> <p>The key aspects shown include:</p> <ul> <li> <strong>Context-aware Generation</strong>: Predicts appropriate continuations based on preceding context</li> <li> <strong>Domain Adaptation</strong>: Shows ability to predict domain-specific terms and technical vocabulary</li> <li> <strong>Sequential Understanding</strong>: Demonstrates understanding of sentence structure and logical flow</li> <li> <strong>Ranking Capabilities</strong>: Provides confidence scores for different prediction options</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Initialize the fill-mask pipeline
</span><span class="n">fill_mask</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">fill-mask</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="sh">"</span><span class="s">answerdotai/ModernBERT-base</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Example text with a masked token at the end
</span><span class="n">masked_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ModernBERT is designed for [MASK].</span><span class="sh">"</span>

<span class="c1"># Get predictions
</span><span class="n">predictions</span> <span class="o">=</span> <span class="nf">fill_mask</span><span class="p">(</span><span class="n">masked_text</span><span class="p">)</span>

<span class="c1"># Display next-word predictions
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Masked Text:</span><span class="sh">"</span><span class="p">,</span> <span class="n">masked_text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next Word Predictions:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  - </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">sequence</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> (score: </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="conclusion">Conclusion</h3> <p>Four approaches were presented showing how ModernBERT can be used to perform various natural language processing tasks without requiring any fine-tuning. Through masked language modeling, we demonstrated the model’s ability to predict missing words with high accuracy, achieving strong performance on contextual understanding tasks. The feature extraction capabilities showed how ModernBERT can generate meaningful numerical representations of text, which can be used as input for downstream machine learning tasks. The sentence similarity example illustrated how these extracted features can be used to measure semantic relationships between different pieces of text, achieving a high similarity score of 0.9572 for semantically related sentences. Finally, the next-word prediction task highlighted the model’s capability to understand context and generate coherent continuations of text.</p> <p>These examples showcase ModernBERT as a general-purpose language model that can handle diverse NLP tasks with minimal setup, making it accessible for both research and practical applications while maintaining high performance across different domains.</p> </body></html>